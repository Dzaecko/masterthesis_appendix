import csv
import subprocess
from colorama import Fore, Style
import datetime

import os
import json
from llama_index import GPTVectorStoreIndex, SimpleDirectoryReader, StorageContext, load_index_from_storage, ServiceContext
from llama_index.llms import OpenAI
import time

 
# ASCII-Art Startbild mit "Dzaecko"
start_art = """
                     ____     ____
                   ,'    `. ,'    `.    ____
                 _/ ,----._/ ,----. \ ,'    `.
                (.|( o  o(.|( o  o )_/ ,----. \_
                  : > '' < : > '' <(.|( o  o )|.)
                   \ ==== / \ {__} / : > '' < ;
                    `.__,'`--`.__,'   \  (_) /
                   ,-'            `--..>.___/
                  /                         `.
                       CVE-LLM-Matcher 0.1    \                                        
"""

print(start_art)  # Display the customized ASCII Art start image

# Path to the CSV file
csv_file = 'cve150.CSV'
persist_dir = "./storage"
data_dir = "./docs"
# Funktion zum Umwandeln des Output-Strings in ein Dictionary
def output_to_dict(output):
    output_dict = {}
    for line in output.split('\n'):
        if '=' in line:
            key, value = line.split('=', 1)
            output_dict[key.strip().replace(' ', '_')] = value.strip()
    return output_dict


def build_storage(data_dir, persist_dir):
    start_time = time.time()
    
    documents = SimpleDirectoryReader(data_dir).load_data()
    total_size_bytes = sum(os.path.getsize(os.path.join(data_dir, doc)) for doc in os.listdir(data_dir) if os.path.isfile(os.path.join(data_dir, doc)))
    number_of_documents = len(documents)
    total_size_mb = total_size_bytes / (1024 ** 2)  # Convert total size to MB
    average_size_mb = total_size_mb / number_of_documents if number_of_documents else 0  # Calculate average size in MB
    
   
    
    if not os.path.exists(persist_dir):
        os.makedirs(persist_dir)
    
    
    index = GPTVectorStoreIndex.from_documents(documents)
    index.storage_context.persist(persist_dir)

    end_time = time.time()
    build_time_seconds = end_time - start_time  # Build time in seconds
    
    stats = {
        'total_documents': number_of_documents,
        'total_size_mb': total_size_mb,  # Total size in MB
        'average_document_size_mb': average_size_mb,  # Average document size in MB
        'build_time_seconds': build_time_seconds  # Build time in seconds
    }
    with open(os.path.join(persist_dir, "stats.json"), "w") as f:
        json.dump(stats, f, indent=4)

    return index

def read_from_storage(persist_dir):
    storage_context = StorageContext.from_defaults(persist_dir=persist_dir)
    return load_index_from_storage(storage_context)

#build_storage(data_dir, persist_dir)
index = read_from_storage(persist_dir)
def adding_data_to_GPT(cve_number):
   
    # if os.path.exists(persist_dir):
    #     index = read_from_storage(persist_dir)
    # else:
    #     index = build_storage(data_dir, persist_dir)
    
    query_engine = index.as_query_engine()
   
    queries = [
     f'You are my assistant to describe weak points correctly. You analyze the data you have and extract the relevant information and prepare it for me. Please provide the JSON data for {cve_number} with the following format: {{"Software": "<software_name>", "Version": "<version_number>", "VulnerabilityType": "<vulnerability_type>"}}.'
     ]
    
    for query in queries:
        response = query_engine.query(query)
        return response



# Funktion zum Speichern der Ergebnisse mit dynamischem Dateinamen
def save_results(output_dict, total_cves_checked):
    today = datetime.date.today().strftime("%Y-%m-%d")
    filename = f"CVE_Vergleich_GPT4_Erkennungsrate_{today}_{total_cves_checked}.csv"
    with open(filename, 'w', newline='') as file:
        csv_writer = csv.writer(file)
        csv_writer.writerow(['CVE-Nummer', 'Erkennungsrate'])  # Header der CSV-Datei
        for cve, data in output_dict.items():
            csv_writer.writerow([cve, data])  # Direktes Speichern der CVE-Nummer und Erkennungsrate

# Dictionary zur Speicherung der Ergebnisse initialisieren
results_dict = {}


with open(csv_file, 'r', newline='') as file:
    csv_reader = csv.reader(file, delimiter=';')
    next(csv_reader)  # Überspringen der Kopfzeile
    for row in csv_reader:
        cve_name = row[0].split(';')[0]
        
        output = adding_data_to_GPT(cve_name)
       
        print(f"{Fore.CYAN}gpt4withData: {Fore.MAGENTA}{cve_name}{Style.RESET_ALL}: {output}")
        print(f"{Fore.GREEN}verified cve {Fore.YELLOW}{row}{Style.RESET_ALL}")        

        result = subprocess.run(['C:\\Users\\Dzaecko\\source\\repos\\MasterThesisScripts\\.venv\\Scripts\\python.exe', 'fuzzyMatcher.py', str(output), str(row)], stdout=subprocess.PIPE, text=True)                
        output = result.stdout.strip()
        
        print("*" * 80)
        print(f"Erkennungsrate von {cve_name} (unter Berücksichtigung von exakten und ähnlichen Übereinstimmungen): {output}")       
        print("*" * 80)
        
        output_dict = output_to_dict(output)
        # Speichern der CVE-Nummer und Erkennungsrate im Dictionary
        erkennungsrate = output_dict.get('Erkennungsrate')  # Angenommen, 'Erkennungsrate' ist der Schlüssel
        if erkennungsrate:
            results_dict[cve_name] = erkennungsrate

# Nachdem alle CVEs verarbeitet wurden, speichern Sie die Ergebnisse
save_results(results_dict, len(results_dict))